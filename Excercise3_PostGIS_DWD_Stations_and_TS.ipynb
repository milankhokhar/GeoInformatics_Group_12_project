{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# geo0930: Insert time series into PostgreSQL/PostGIS and join it with the station info geodata.\n",
    "\n",
    "The main idea behind this activity is to reformat and merge time series (here we use hourly precipitation) as well as weather station information from the DWD Climate Data Center in such a way that it can be used with the **QGIS TimeManager extension**. But this time the **join** of station info geodata and time series are performed in **PostgreSQL/PostGIS** instead of Pandas and CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = \"opendata.dwd.de\"\n",
    "user   = \"anonymous\"\n",
    "passwd = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTP Directory Definition and Station Description Filename Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The topic of interest.\n",
    "topic_dir = \"/hourly/precipitation/historical/\"\n",
    "#topic_dir = \"/annual/kl/historical/\"\n",
    "\n",
    "# This is the search pattern common to ALL station description file names \n",
    "station_desc_pattern = \"_Beschreibung_Stationen.txt\"\n",
    "\n",
    "# Below this directory tree node all climate data are stored.\n",
    "ftp_climate_data_dir = \"/climate_environment/CDC/observations_germany/climate/\"\n",
    "ftp_dir =  ftp_climate_data_dir + topic_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Create Local Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_ftp_dir         = \"data/original/DWD/\"      # Local directory to store local ftp data copies, the local data source or input data. \n",
    "local_ftp_station_dir = local_ftp_dir + topic_dir # Local directory where local station info is located\n",
    "local_ftp_ts_dir      = local_ftp_dir + topic_dir # Local directory where time series downloaded from ftp are located\n",
    "\n",
    "local_generated_dir   = \"data/generated/DWD/\" # The generated of derived data in contrast to local_ftp_dir\n",
    "local_station_dir     = local_generated_dir + topic_dir # Derived station data, i.e. the CSV file\n",
    "local_ts_merged_dir   = local_generated_dir + topic_dir # Parallelly merged time series, wide data frame with one TS per column\n",
    "local_ts_appended_dir = local_generated_dir + topic_dir # Serially appended time series, long data frame for QGIS TimeManager Plugin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/original/DWD/\n",
      "data/original/DWD//hourly/precipitation/historical/\n",
      "data/original/DWD//hourly/precipitation/historical/\n",
      "\n",
      "data/generated/DWD/\n",
      "data/generated/DWD//hourly/precipitation/historical/\n",
      "data/generated/DWD//hourly/precipitation/historical/\n",
      "data/generated/DWD//hourly/precipitation/historical/\n"
     ]
    }
   ],
   "source": [
    "print(local_ftp_dir) #generated in GIS_ExamWiSe2020-21>Repo\n",
    "print(local_ftp_station_dir)\n",
    "print(local_ftp_ts_dir)\n",
    "print()\n",
    "print(local_generated_dir)\n",
    "print(local_station_dir)\n",
    "print(local_ts_merged_dir)\n",
    "print(local_ts_appended_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(local_ftp_dir,exist_ok = True) # it does not complain if the dir already exists.\n",
    "os.makedirs(local_ftp_station_dir,exist_ok = True)\n",
    "os.makedirs(local_ftp_ts_dir,exist_ok = True)\n",
    "\n",
    "os.makedirs(local_generated_dir,exist_ok = True)\n",
    "os.makedirs(local_station_dir,exist_ok = True)\n",
    "os.makedirs(local_ts_merged_dir,exist_ok = True)\n",
    "os.makedirs(local_ts_appended_dir,exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTP Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 Login successful.\n"
     ]
    }
   ],
   "source": [
    "import ftplib\n",
    "ftp = ftplib.FTP(server)\n",
    "res = ftp.login(user=user, passwd = passwd)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = ftp.cwd(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ftp.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Pandas Dataframe from FTP Directory Listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>name</th>\n",
       "      <th>ext</th>\n",
       "      <th>size</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>BESCHREIBUNG_obsgermany_climate_hourly_precipi...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>166317</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>DESCRIPTION_obsgermany_climate_hourly_precipit...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>161348</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>RR_Stundenwerte_Beschreibung_Stationen.txt</td>\n",
       "      <td>.txt</td>\n",
       "      <td>303009</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>stundenwerte_RR_00003_19950901_20110401_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>419296</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>stundenwerte_RR_00020_20040814_20201231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>432124</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id                                               name   ext  \\\n",
       "0          -1  BESCHREIBUNG_obsgermany_climate_hourly_precipi...  .pdf   \n",
       "1          -1  DESCRIPTION_obsgermany_climate_hourly_precipit...  .pdf   \n",
       "2          -1         RR_Stundenwerte_Beschreibung_Stationen.txt  .txt   \n",
       "3           3   stundenwerte_RR_00003_19950901_20110401_hist.zip  .zip   \n",
       "4          20   stundenwerte_RR_00020_20040814_20201231_hist.zip  .zip   \n",
       "\n",
       "     size type  \n",
       "0  166317    -  \n",
       "1  161348    -  \n",
       "2  303009    -  \n",
       "3  419296    -  \n",
       "4  432124    -  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import my_dwd.py\n",
    "from my_dwd import gen_df_from_ftp_dir_listing\n",
    "df_ftpdir = gen_df_from_ftp_dir_listing(ftp, ftp_dir)\n",
    "df_ftpdir.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Process the Station Description File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab the txt File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dwd import grabFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station description file name:\n",
      "RR_Stundenwerte_Beschreibung_Stationen.txt\n"
     ]
    }
   ],
   "source": [
    "station_fname = df_ftpdir[df_ftpdir['name'].str.contains(station_desc_pattern)][\"name\"].values[0]\n",
    "print(\"Station description file name:\\n%s\" % (station_fname))\n",
    "\n",
    "# ALternative\n",
    "#station_fname2 = df_ftpdir[df_ftpdir[\"name\"].str.match(\"^.*Beschreibung_Stationen.*txt$\")][\"name\"].values[0]\n",
    "#print(station_fname2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grabFile(ftp, src, dest):\n",
      "FTP source: /climate_environment/CDC/observations_germany/climate//hourly/precipitation/historical/RR_Stundenwerte_Beschreibung_Stationen.txt\n",
      "Local dest:   data/original/DWD//hourly/precipitation/historical/RR_Stundenwerte_Beschreibung_Stationen.txt\n"
     ]
    }
   ],
   "source": [
    "src = ftp_dir + station_fname\n",
    "dest = local_ftp_station_dir + station_fname\n",
    "print(\"grabFile(ftp, src, dest):\")\n",
    "print(\"FTP source: \" + src)\n",
    "print(\"Local dest:   \" + dest)\n",
    "grabFile(ftp, src, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract column names. They are in German (de)\n",
    "# We have to use codecs because of difficulties with character encoding (German Umlaute)\n",
    "import codecs\n",
    "\n",
    "def station_desc_txt_to_csv(txtfile, csvfile):\n",
    "    import pandas as pd\n",
    "    \n",
    "    file = codecs.open(txtfile,\"r\",\"utf-8\") \n",
    "    r = file.readline()\n",
    "    file.close()\n",
    "    colnames_de = r.split()\n",
    "    colnames_de\n",
    "    \n",
    "    # German-English dictionary\n",
    "    translate = \\\n",
    "    {'Stations_id':'station_id',\n",
    "     'von_datum':'date_from',\n",
    "     'bis_datum':'date_to',\n",
    "     'Stationshoehe':'altitude',\n",
    "     'geoBreite': 'latitude',\n",
    "     'geoLaenge': 'longitude',\n",
    "     'Stationsname':'name',\n",
    "     'Bundesland':'state'}\n",
    "    \n",
    "    colnames_en = [translate[h] for h in colnames_de]\n",
    "    \n",
    "    # Skip the first two rows and set the column names.\n",
    "    df = pd.read_fwf(txtfile,skiprows=2,names=colnames_en, parse_dates=[\"date_from\",\"date_to\"],index_col = 0, encoding=\"unicode-escape\")\n",
    "    \n",
    "    # write CSV file with field separator semicolon\n",
    "    df.to_csv(csvfile, sep = \";\")\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename the Column Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>altitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1995-09-01</td>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>202</td>\n",
       "      <td>50.7827</td>\n",
       "      <td>6.0941</td>\n",
       "      <td>Aachen</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2004-08-14</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>432</td>\n",
       "      <td>48.9219</td>\n",
       "      <td>9.9129</td>\n",
       "      <td>Abtsgmünd-Untergröningen</td>\n",
       "      <td>Baden-Württemberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2006-01-10</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>260</td>\n",
       "      <td>49.7175</td>\n",
       "      <td>10.9101</td>\n",
       "      <td>Adelsdorf (Kläranlage)</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2007-04-01</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>44</td>\n",
       "      <td>52.9336</td>\n",
       "      <td>8.2370</td>\n",
       "      <td>Großenkneten</td>\n",
       "      <td>Niedersachsen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2006-01-03</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>325</td>\n",
       "      <td>48.9450</td>\n",
       "      <td>12.4639</td>\n",
       "      <td>Aholfing</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_from    date_to  altitude  latitude  longitude  \\\n",
       "station_id                                                        \n",
       "3          1995-09-01 2011-04-01       202   50.7827     6.0941   \n",
       "20         2004-08-14 2022-03-03       432   48.9219     9.9129   \n",
       "29         2006-01-10 2022-03-03       260   49.7175    10.9101   \n",
       "44         2007-04-01 2022-03-03        44   52.9336     8.2370   \n",
       "46         2006-01-03 2022-03-03       325   48.9450    12.4639   \n",
       "\n",
       "                                name                state  \n",
       "station_id                                                 \n",
       "3                             Aachen  Nordrhein-Westfalen  \n",
       "20          Abtsgmünd-Untergröningen    Baden-Württemberg  \n",
       "29            Adelsdorf (Kläranlage)               Bayern  \n",
       "44                      Großenkneten        Niedersachsen  \n",
       "46                          Aholfing               Bayern  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from my_dwd import station_desc_txt_to_csv\n",
    "basename = os.path.splitext(station_fname)[0]\n",
    "df_stations = station_desc_txt_to_csv(local_ftp_station_dir + station_fname, local_station_dir + basename + \".csv\")\n",
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only Stations Located in NRW and being Operational in 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>altitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>2004-10-01</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>298</td>\n",
       "      <td>51.1143</td>\n",
       "      <td>7.8807</td>\n",
       "      <td>Attendorn-Neulisternohl</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>2009-11-01</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>436</td>\n",
       "      <td>51.0148</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>Berleburg, Bad-Arfeld</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>2004-07-01</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>610</td>\n",
       "      <td>50.9837</td>\n",
       "      <td>8.3683</td>\n",
       "      <td>Berleburg, Bad-Stünzel</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>1995-09-01</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>23</td>\n",
       "      <td>51.8293</td>\n",
       "      <td>6.5365</td>\n",
       "      <td>Bocholt-Liedern (Wasserwerk)</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>2008-01-01</td>\n",
       "      <td>2018-11-01</td>\n",
       "      <td>101</td>\n",
       "      <td>51.4789</td>\n",
       "      <td>7.2697</td>\n",
       "      <td>Bochum</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14184</th>\n",
       "      <td>2016-06-01</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>126</td>\n",
       "      <td>51.2242</td>\n",
       "      <td>7.1070</td>\n",
       "      <td>Wuppertal-Buchenhofen/Wupper</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14185</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>304</td>\n",
       "      <td>51.6050</td>\n",
       "      <td>8.8175</td>\n",
       "      <td>Lichtenau-Ebbinghausen (HRB)</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14186</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>291</td>\n",
       "      <td>51.5319</td>\n",
       "      <td>8.7289</td>\n",
       "      <td>Gollentaler Grund (HRB)</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14187</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>2021-01-07</td>\n",
       "      <td>226</td>\n",
       "      <td>51.5831</td>\n",
       "      <td>8.8478</td>\n",
       "      <td>Husen-Dalheim (HRB)</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>231</td>\n",
       "      <td>50.7983</td>\n",
       "      <td>6.0244</td>\n",
       "      <td>Aachen-Orsbach</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_from    date_to  altitude  latitude  longitude  \\\n",
       "station_id                                                        \n",
       "216        2004-10-01 2022-02-25       298   51.1143     7.8807   \n",
       "389        2009-11-01 2022-03-03       436   51.0148     8.4318   \n",
       "390        2004-07-01 2022-03-03       610   50.9837     8.3683   \n",
       "554        1995-09-01 2022-03-03        23   51.8293     6.5365   \n",
       "555        2008-01-01 2018-11-01       101   51.4789     7.2697   \n",
       "...               ...        ...       ...       ...        ...   \n",
       "14184      2016-06-01 2022-03-03       126   51.2242     7.1070   \n",
       "14185      2017-08-01 2022-03-03       304   51.6050     8.8175   \n",
       "14186      2017-08-01 2022-03-03       291   51.5319     8.7289   \n",
       "14187      2017-08-01 2021-01-07       226   51.5831     8.8478   \n",
       "15000      2011-04-01 2022-03-03       231   50.7983     6.0244   \n",
       "\n",
       "                                    name                state  \n",
       "station_id                                                     \n",
       "216              Attendorn-Neulisternohl  Nordrhein-Westfalen  \n",
       "389                Berleburg, Bad-Arfeld  Nordrhein-Westfalen  \n",
       "390               Berleburg, Bad-Stünzel  Nordrhein-Westfalen  \n",
       "554         Bocholt-Liedern (Wasserwerk)  Nordrhein-Westfalen  \n",
       "555                               Bochum  Nordrhein-Westfalen  \n",
       "...                                  ...                  ...  \n",
       "14184       Wuppertal-Buchenhofen/Wupper  Nordrhein-Westfalen  \n",
       "14185       Lichtenau-Ebbinghausen (HRB)  Nordrhein-Westfalen  \n",
       "14186            Gollentaler Grund (HRB)  Nordrhein-Westfalen  \n",
       "14187                Husen-Dalheim (HRB)  Nordrhein-Westfalen  \n",
       "15000                     Aachen-Orsbach  Nordrhein-Westfalen  \n",
       "\n",
       "[153 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create variable with TRUE if state is Nordrhein-Westfalen\n",
    "isNRW = df_stations['state'].str.contains(\"Nordrhein\")\n",
    "\n",
    "# Create variable with TRUE if date_to is latest date (indicates operation up to now) and date_from is 2018 and before.\n",
    "isOperational = (df_stations['date_to'] >= '2018') &  (df_stations['date_from'] <= '2018')\n",
    "\n",
    "# select on both conditions\n",
    "dfNRW = df_stations[isNRW & isOperational]\n",
    "\n",
    "#print(\"Number of stations in NRW: \\n\", dfNRW.count())\n",
    "dfNRW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geopandas - Create a Geo Data Frame\n",
    "\n",
    "A Geopandas geo data frame is a Pandas data frame enriched with an additional geometry column. Each row in the data frame becomes a location information. Thus a geo-df contains geometry and attributes, i.e. full features. The geo-df is self-contained and complete. It can be easily saved in different vectore file formats, i.e. shapefile or geopackage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>altitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>2004-10-01</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>298</td>\n",
       "      <td>51.1143</td>\n",
       "      <td>7.8807</td>\n",
       "      <td>Attendorn-Neulisternohl</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "      <td>POINT (7.88070 51.11430)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>2009-11-01</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>436</td>\n",
       "      <td>51.0148</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>Berleburg, Bad-Arfeld</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "      <td>POINT (8.43180 51.01480)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>2004-07-01</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>610</td>\n",
       "      <td>50.9837</td>\n",
       "      <td>8.3683</td>\n",
       "      <td>Berleburg, Bad-Stünzel</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "      <td>POINT (8.36830 50.98370)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>1995-09-01</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>23</td>\n",
       "      <td>51.8293</td>\n",
       "      <td>6.5365</td>\n",
       "      <td>Bocholt-Liedern (Wasserwerk)</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "      <td>POINT (6.53650 51.82930)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>2008-01-01</td>\n",
       "      <td>2018-11-01</td>\n",
       "      <td>101</td>\n",
       "      <td>51.4789</td>\n",
       "      <td>7.2697</td>\n",
       "      <td>Bochum</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "      <td>POINT (7.26970 51.47890)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_from    date_to  altitude  latitude  longitude  \\\n",
       "station_id                                                        \n",
       "216        2004-10-01 2022-02-25       298   51.1143     7.8807   \n",
       "389        2009-11-01 2022-03-03       436   51.0148     8.4318   \n",
       "390        2004-07-01 2022-03-03       610   50.9837     8.3683   \n",
       "554        1995-09-01 2022-03-03        23   51.8293     6.5365   \n",
       "555        2008-01-01 2018-11-01       101   51.4789     7.2697   \n",
       "\n",
       "                                    name                state  \\\n",
       "station_id                                                      \n",
       "216              Attendorn-Neulisternohl  Nordrhein-Westfalen   \n",
       "389                Berleburg, Bad-Arfeld  Nordrhein-Westfalen   \n",
       "390               Berleburg, Bad-Stünzel  Nordrhein-Westfalen   \n",
       "554         Bocholt-Liedern (Wasserwerk)  Nordrhein-Westfalen   \n",
       "555                               Bochum  Nordrhein-Westfalen   \n",
       "\n",
       "                            geometry  \n",
       "station_id                            \n",
       "216         POINT (7.88070 51.11430)  \n",
       "389         POINT (8.43180 51.01480)  \n",
       "390         POINT (8.36830 50.98370)  \n",
       "554         POINT (6.53650 51.82930)  \n",
       "555         POINT (7.26970 51.47890)  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "import fiona\n",
    "from pyproj import CRS\n",
    "\n",
    "#df = pd.read_csv('data.csv')\n",
    "df = dfNRW\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(df.longitude, df.latitude)]\n",
    "crs = CRS(\"epsg:4326\") #http://www.spatialreference.org/ref/epsg/2263/\n",
    "stations_gdf = gpd.GeoDataFrame(df, crs=crs, geometry=geometry)\n",
    "\n",
    "stations_gdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the PostGIS database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection URL:  postgresql://geo_master:xxxxxx@localhost:5432/geo\n"
     ]
    }
   ],
   "source": [
    "# PostgreSQL connection parameters -> create connection string (URL) \n",
    "\n",
    "param_dic = {\n",
    "  \"user\" : \"geo_master\",\n",
    "  \"pw\"   : \"xxxxxx\",\n",
    "  \"host\" : \"localhost\",\n",
    "  \"db\"   : \"geo\"\n",
    "}\n",
    "\n",
    "# https://www.w3schools.com/python/ref_string_format.asp\n",
    "template = \"postgresql://{user}:{pw}@{host}:5432/{db}\"\n",
    "\n",
    "db_connection_url = template.format(**param_dic)\n",
    "print(\"Connection URL: \", db_connection_url) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Geopandas Data Frame directly into PostGIS Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x1e3adcac0a0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import Numeric, Float, Date, REAL\n",
    "\n",
    "engine = create_engine(db_connection_url)\n",
    "\n",
    "\n",
    "# Set data types in PG explicitly.\n",
    "dtypes = {\"station_id\": Numeric(6,0), \"altitude\" : REAL, \"date_from\" : Date, \"date_to\" : Date, \"longitude\" : REAL, \"latitude\" : REAL}\n",
    "engine.execute(\"DROP VIEW IF EXISTS dwd.v_stations_prec\")\n",
    "\n",
    "stations_gdf.to_postgis(name=\"stations\", schema=\"dwd\", if_exists = \"replace\", index = \"station_id\", index_label=True, con=engine, dtype=dtypes)\n",
    "\n",
    "#engine.execute('alter table dwd.stations add constraint my_awesome_pkey primary key (station_id)')\n",
    "engine.execute('alter table dwd.stations add primary key (station_id)') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Process the Time Series Zip Archives\n",
    "\n",
    "Extract the product file (txt file containing several time series for different variables) from an archive, extract the relevant time series from the product file, limit the time series interval if needed and append it to a dataframe. Finally insert the dataframe to the PostGIS database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe with TS Zip Files from FTP Directory Listing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ext</th>\n",
       "      <th>size</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stundenwerte_RR_00003_19950901_20110401_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>419296</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>stundenwerte_RR_00020_20040814_20201231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>432124</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>stundenwerte_RR_00044_20070401_20201231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>354983</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>stundenwerte_RR_00053_20051001_20201231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>385830</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>stundenwerte_RR_00071_20041022_20200101_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>402875</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        name   ext    size  \\\n",
       "station_id                                                                   \n",
       "3           stundenwerte_RR_00003_19950901_20110401_hist.zip  .zip  419296   \n",
       "20          stundenwerte_RR_00020_20040814_20201231_hist.zip  .zip  432124   \n",
       "44          stundenwerte_RR_00044_20070401_20201231_hist.zip  .zip  354983   \n",
       "53          stundenwerte_RR_00053_20051001_20201231_hist.zip  .zip  385830   \n",
       "71          stundenwerte_RR_00071_20041022_20200101_hist.zip  .zip  402875   \n",
       "\n",
       "           type  \n",
       "station_id       \n",
       "3             -  \n",
       "20            -  \n",
       "44            -  \n",
       "53            -  \n",
       "71            -  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_ftpdir[\"ext\"]==\".zip\"\n",
    "df_zips = df_ftpdir[df_ftpdir[\"ext\"]==\".zip\"]\n",
    "df_zips.set_index(\"station_id\", inplace = True)\n",
    "df_zips.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download TS Data from FTP Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stundenwerte_RR_00216_20041001_20201231_hist.zip\n",
      "stundenwerte_RR_00389_20091101_20201231_hist.zip\n",
      "stundenwerte_RR_00390_20040701_20201231_hist.zip\n",
      "stundenwerte_RR_00554_19950901_20201231_hist.zip\n",
      "stundenwerte_RR_00555_20080101_20181101_hist.zip\n",
      "stundenwerte_RR_00603_19990303_20201231_hist.zip\n",
      "stundenwerte_RR_00613_20041101_20201231_hist.zip\n",
      "stundenwerte_RR_00617_20040601_20201231_hist.zip\n",
      "stundenwerte_RR_00644_20050101_20201231_hist.zip\n",
      "stundenwerte_RR_00796_20041101_20201231_hist.zip\n",
      "stundenwerte_RR_00871_20050801_20201231_hist.zip\n",
      "stundenwerte_RR_00902_20061001_20201231_hist.zip\n",
      "stundenwerte_RR_00934_20041001_20201231_hist.zip\n",
      "stundenwerte_RR_00989_20050201_20201231_hist.zip\n",
      "stundenwerte_RR_01024_20060801_20201231_hist.zip\n",
      "stundenwerte_RR_01046_20041001_20201231_hist.zip\n",
      "stundenwerte_RR_01078_19950901_20201231_hist.zip\n",
      "stundenwerte_RR_01241_20061201_20201231_hist.zip\n",
      "stundenwerte_RR_01246_20150801_20201231_hist.zip\n",
      "stundenwerte_RR_01300_20040601_20201231_hist.zip\n",
      "stundenwerte_RR_01303_19950901_20201231_hist.zip\n",
      "stundenwerte_RR_01327_20040801_20201231_hist.zip\n",
      "stundenwerte_RR_01590_20030701_20201231_hist.zip\n",
      "stundenwerte_RR_01595_20121001_20201231_hist.zip\n",
      "stundenwerte_RR_01766_19950901_20201231_hist.zip\n",
      "stundenwerte_RR_02027_20060601_20201231_hist.zip\n",
      "stundenwerte_RR_02110_20030105_20201231_hist.zip\n",
      "stundenwerte_RR_02254_20050601_20201231_hist.zip\n",
      "stundenwerte_RR_02473_20041201_20201231_hist.zip\n",
      "stundenwerte_RR_02483_19951012_20201231_hist.zip\n",
      "stundenwerte_RR_02497_20040801_20201231_hist.zip\n",
      "stundenwerte_RR_02629_20040701_20201231_hist.zip\n",
      "stundenwerte_RR_02667_19950901_20201231_hist.zip\n",
      "stundenwerte_RR_02810_20061201_20201231_hist.zip\n",
      "stundenwerte_RR_02947_20061001_20201231_hist.zip\n",
      "stundenwerte_RR_02968_20081201_20201231_hist.zip\n",
      "stundenwerte_RR_02999_20040701_20201231_hist.zip\n",
      "stundenwerte_RR_03028_19950901_20201231_hist.zip\n",
      "stundenwerte_RR_03031_20040701_20201231_hist.zip\n",
      "stundenwerte_RR_03081_20071201_20201231_hist.zip\n",
      "stundenwerte_RR_03098_19950901_20201231_hist.zip\n",
      "stundenwerte_RR_03215_20070601_20201231_hist.zip\n",
      "stundenwerte_RR_03321_20050701_20201231_hist.zip\n",
      "WARNING: TS file for key 3328 not found in FTP directory.\n",
      "stundenwerte_RR_03339_20060901_20201231_hist.zip\n",
      "stundenwerte_RR_03499_20060701_20201231_hist.zip\n",
      "stundenwerte_RR_03540_20041101_20201231_hist.zip\n",
      "stundenwerte_RR_03591_20040601_20201231_hist.zip\n",
      "stundenwerte_RR_03795_20041201_20201231_hist.zip\n",
      "stundenwerte_RR_03913_20040701_20201231_hist.zip\n",
      "stundenwerte_RR_04063_20030701_20201231_hist.zip\n",
      "stundenwerte_RR_04127_20050101_20201231_hist.zip\n",
      "stundenwerte_RR_04150_20051201_20201231_hist.zip\n",
      "stundenwerte_RR_04154_20110201_20180701_hist.zip\n",
      "stundenwerte_RR_04313_20040801_20201231_hist.zip\n",
      "stundenwerte_RR_04368_20041001_20201231_hist.zip\n",
      "stundenwerte_RR_04371_19950901_20201231_hist.zip\n",
      "stundenwerte_RR_04400_20040801_20201231_hist.zip\n",
      "stundenwerte_RR_04488_20060801_20201231_hist.zip\n",
      "stundenwerte_RR_04692_20080301_20181201_hist.zip\n",
      "stundenwerte_RR_04741_20041201_20201231_hist.zip\n",
      "stundenwerte_RR_04849_20050601_20201231_hist.zip\n",
      "stundenwerte_RR_05064_20041201_20201231_hist.zip\n",
      "stundenwerte_RR_05347_19950901_20201231_hist.zip\n",
      "stundenwerte_RR_05360_20070701_20201231_hist.zip\n",
      "stundenwerte_RR_05468_20060701_20201001_hist.zip\n",
      "stundenwerte_RR_05480_20030910_20201231_hist.zip\n",
      "stundenwerte_RR_05513_20050901_20201231_hist.zip\n",
      "stundenwerte_RR_05619_20041201_20201231_hist.zip\n",
      "stundenwerte_RR_05699_20041101_20201231_hist.zip\n",
      "stundenwerte_RR_05717_20060901_20201231_hist.zip\n",
      "stundenwerte_RR_05733_20050501_20201231_hist.zip\n",
      "WARNING: TS file for key 6041 not found in FTP directory.\n",
      "WARNING: TS file for key 6042 not found in FTP directory.\n",
      "WARNING: TS file for key 6043 not found in FTP directory.\n",
      "WARNING: TS file for key 6044 not found in FTP directory.\n",
      "WARNING: TS file for key 6045 not found in FTP directory.\n",
      "WARNING: TS file for key 6046 not found in FTP directory.\n",
      "WARNING: TS file for key 6047 not found in FTP directory.\n",
      "WARNING: TS file for key 6048 not found in FTP directory.\n",
      "WARNING: TS file for key 6049 not found in FTP directory.\n",
      "WARNING: TS file for key 6050 not found in FTP directory.\n",
      "WARNING: TS file for key 6051 not found in FTP directory.\n",
      "WARNING: TS file for key 6052 not found in FTP directory.\n",
      "WARNING: TS file for key 6053 not found in FTP directory.\n",
      "WARNING: TS file for key 6054 not found in FTP directory.\n",
      "WARNING: TS file for key 6055 not found in FTP directory.\n",
      "WARNING: TS file for key 6057 not found in FTP directory.\n",
      "WARNING: TS file for key 6058 not found in FTP directory.\n",
      "WARNING: TS file for key 6059 not found in FTP directory.\n",
      "WARNING: TS file for key 6060 not found in FTP directory.\n",
      "WARNING: TS file for key 6061 not found in FTP directory.\n",
      "WARNING: TS file for key 6064 not found in FTP directory.\n",
      "WARNING: TS file for key 6067 not found in FTP directory.\n",
      "stundenwerte_RR_06197_20001013_20201231_hist.zip\n",
      "stundenwerte_RR_06264_20040601_20201231_hist.zip\n",
      "stundenwerte_RR_06276_20041101_20191102_hist.zip\n",
      "stundenwerte_RR_06313_20041201_20201231_hist.zip\n",
      "stundenwerte_RR_06337_20040801_20201231_hist.zip\n",
      "stundenwerte_RR_07106_20060901_20201231_hist.zip\n",
      "stundenwerte_RR_07330_20051001_20201231_hist.zip\n",
      "stundenwerte_RR_07344_20060601_20201231_hist.zip\n",
      "stundenwerte_RR_07374_20060301_20201231_hist.zip\n",
      "stundenwerte_RR_07378_20060701_20201231_hist.zip\n",
      "stundenwerte_RR_13669_20070901_20201231_hist.zip\n",
      "stundenwerte_RR_13670_20070601_20201231_hist.zip\n",
      "stundenwerte_RR_13671_20071201_20201231_hist.zip\n",
      "stundenwerte_RR_13696_20071201_20201231_hist.zip\n",
      "stundenwerte_RR_13700_20080501_20201231_hist.zip\n",
      "stundenwerte_RR_13713_20071101_20201231_hist.zip\n",
      "WARNING: TS file for key 14096 not found in FTP directory.\n",
      "WARNING: TS file for key 14142 not found in FTP directory.\n",
      "WARNING: TS file for key 14143 not found in FTP directory.\n",
      "WARNING: TS file for key 14144 not found in FTP directory.\n",
      "WARNING: TS file for key 14145 not found in FTP directory.\n",
      "WARNING: TS file for key 14146 not found in FTP directory.\n",
      "WARNING: TS file for key 14147 not found in FTP directory.\n",
      "WARNING: TS file for key 14148 not found in FTP directory.\n",
      "WARNING: TS file for key 14149 not found in FTP directory.\n",
      "WARNING: TS file for key 14150 not found in FTP directory.\n",
      "WARNING: TS file for key 14151 not found in FTP directory.\n",
      "WARNING: TS file for key 14152 not found in FTP directory.\n",
      "WARNING: TS file for key 14153 not found in FTP directory.\n",
      "WARNING: TS file for key 14154 not found in FTP directory.\n",
      "WARNING: TS file for key 14155 not found in FTP directory.\n",
      "WARNING: TS file for key 14156 not found in FTP directory.\n",
      "WARNING: TS file for key 14158 not found in FTP directory.\n",
      "WARNING: TS file for key 14159 not found in FTP directory.\n",
      "WARNING: TS file for key 14164 not found in FTP directory.\n",
      "WARNING: TS file for key 14165 not found in FTP directory.\n",
      "WARNING: TS file for key 14166 not found in FTP directory.\n",
      "WARNING: TS file for key 14167 not found in FTP directory.\n",
      "WARNING: TS file for key 14168 not found in FTP directory.\n",
      "WARNING: TS file for key 14169 not found in FTP directory.\n",
      "WARNING: TS file for key 14170 not found in FTP directory.\n",
      "WARNING: TS file for key 14171 not found in FTP directory.\n",
      "WARNING: TS file for key 14172 not found in FTP directory.\n",
      "WARNING: TS file for key 14173 not found in FTP directory.\n",
      "WARNING: TS file for key 14174 not found in FTP directory.\n",
      "WARNING: TS file for key 14175 not found in FTP directory.\n",
      "WARNING: TS file for key 14176 not found in FTP directory.\n",
      "WARNING: TS file for key 14177 not found in FTP directory.\n",
      "WARNING: TS file for key 14178 not found in FTP directory.\n",
      "WARNING: TS file for key 14179 not found in FTP directory.\n",
      "WARNING: TS file for key 14180 not found in FTP directory.\n",
      "WARNING: TS file for key 14181 not found in FTP directory.\n",
      "WARNING: TS file for key 14182 not found in FTP directory.\n",
      "WARNING: TS file for key 14183 not found in FTP directory.\n",
      "WARNING: TS file for key 14184 not found in FTP directory.\n",
      "WARNING: TS file for key 14185 not found in FTP directory.\n",
      "WARNING: TS file for key 14186 not found in FTP directory.\n",
      "WARNING: TS file for key 14187 not found in FTP directory.\n",
      "stundenwerte_RR_15000_20110401_20201231_hist.zip\n"
     ]
    }
   ],
   "source": [
    "# Add the names of the actually downloaded zip files to this list. \n",
    "local_zip_list = []\n",
    "\n",
    "station_ids_selected = list(dfNRW.index)\n",
    "\n",
    "for station_id in station_ids_selected:\n",
    "    try:\n",
    "        fname = df_zips[\"name\"][station_id]\n",
    "        print(fname)\n",
    "        grabFile(ftp, ftp_dir + fname, local_ftp_ts_dir + fname)\n",
    "        local_zip_list.append(fname)\n",
    "    except:\n",
    "        print(\"WARNING: TS file for key %d not found in FTP directory.\" % station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(local_zip_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to extract PRECIPITATION from recent and historical RR product files.\n",
    "def prec_ts_to_df(fname):\n",
    "\n",
    "    import pandas as pd\n",
    "    import pytz\n",
    "    from datetime import datetime\n",
    "    \n",
    "    dateparse = lambda dates: [datetime.strptime(str(d), '%Y%m%d%H') for d in dates]\n",
    "\n",
    "    df = pd.read_csv(fname, delimiter=\";\", encoding=\"utf8\", index_col=\"MESS_DATUM\", parse_dates = [\"MESS_DATUM\"], date_parser = dateparse, na_values = [-999.0, -999])\n",
    "    \n",
    "    # https://medium.com/@chaimgluck1/working-with-pandas-fixing-messy-column-names-42a54a6659cd\n",
    "\n",
    "    # Column headers: remove leading blanks (strip), replace \" \" with \"_\", and convert to lower case.\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_', regex=True).str.replace('(', '', regex=True).str.replace(')', '', regex=True)\n",
    "    df.index.name = df.index.name.strip().lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    \n",
    "    # TIME ZONES: https://stackoverflow.com/questions/22800079/converting-time-zone-pandas-dataframe\n",
    "    # DWD Prec Data is given in UTC\n",
    "    df.index = df.index.tz_localize(pytz.utc)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_00216_20041001_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20041001_20201231_00216.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_00389_20091101_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20091101_20201231_00389.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_00390_20040701_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20040701_20201231_00390.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_00554_19950901_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_00554.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_00555_20080101_20181101_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20080101_20181101_00555.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_00603_19990303_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_19990303_20201231_00603.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_00613_20041101_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20041101_20201231_00613.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_00617_20040601_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20040601_20201231_00617.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_00644_20050101_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20050101_20201231_00644.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_00796_20041101_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20041101_20201231_00796.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_00871_20050801_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20050801_20201231_00871.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_00902_20061001_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20061001_20201231_00902.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_00934_20041001_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20041001_20201231_00934.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_00989_20050201_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20050201_20201231_00989.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_01024_20060801_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20060801_20201231_01024.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_01046_20041001_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20041001_20201231_01046.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_01078_19950901_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_01078.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_01241_20061201_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20061201_20201231_01241.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_01246_20150801_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20150801_20201231_01246.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_01300_20040601_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20040601_20201231_01300.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_01303_19950901_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_01303.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_01327_20040801_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20040801_20201231_01327.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_01590_20030701_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20030701_20201231_01590.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_01595_20121001_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20121001_20201231_01595.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_01766_19950901_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_01766.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_02027_20060601_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20060601_20201231_02027.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_02110_20030105_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20030105_20201231_02110.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_02254_20050601_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20050601_20201231_02254.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_02473_20041201_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20041201_20201231_02473.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_02483_19951012_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_19951012_20201231_02483.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_02497_20040801_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20040801_20201231_02497.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_02629_20040701_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20040701_20201231_02629.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_02667_19950901_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_02667.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_02810_20061201_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20061201_20201231_02810.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_02947_20061001_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20061001_20201231_02947.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_02968_20081201_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20081201_20201231_02968.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_02999_20040701_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20040701_20201231_02999.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_03028_19950901_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_03028.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_03031_20040701_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20040701_20201231_03031.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_03081_20071201_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20071201_20201231_03081.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_03098_19950901_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_03098.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_03215_20070601_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20070601_20201231_03215.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_03321_20050701_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20050701_20201231_03321.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_03339_20060901_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20060901_20201231_03339.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_03499_20060701_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20060701_20201231_03499.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_03540_20041101_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20041101_20201231_03540.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_03591_20040601_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20040601_20201231_03591.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_03795_20041201_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20041201_20201231_03795.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_03913_20040701_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20040701_20201231_03913.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_04063_20030701_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20030701_20201231_04063.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_04127_20050101_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20050101_20201231_04127.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_04150_20051201_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20051201_20201231_04150.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_04154_20110201_20180701_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20110201_20180701_04154.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_04313_20040801_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20040801_20201231_04313.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_04368_20041001_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20041001_20201231_04368.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_04371_19950901_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_04371.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_04400_20040801_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20040801_20201231_04400.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_04488_20060801_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20060801_20201231_04488.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_04692_20080301_20181201_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20080301_20181201_04692.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_04741_20041201_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20041201_20201231_04741.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_04849_20050601_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20050601_20201231_04849.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_05064_20041201_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20041201_20201231_05064.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_05347_19950901_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_05347.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_05360_20070701_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20070701_20201231_05360.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_05468_20060701_20201001_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20060701_20201001_05468.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_05480_20030910_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20030910_20201231_05480.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_05513_20050901_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20050901_20201231_05513.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_05619_20041201_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20041201_20201231_05619.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_05699_20041101_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20041101_20201231_05699.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_05717_20060901_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20060901_20201231_05717.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_05733_20050501_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20050501_20201231_05733.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_06197_20001013_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20001013_20201231_06197.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_06264_20040601_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20040601_20201231_06264.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_06276_20041101_20191102_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20041101_20191102_06276.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_06313_20041201_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20041201_20201231_06313.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_06337_20040801_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20040801_20201231_06337.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_07106_20060901_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20060901_20201231_07106.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_07330_20051001_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20051001_20201231_07330.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_07344_20060601_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20060601_20201231_07344.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_07374_20060301_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20060301_20201231_07374.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_07378_20060701_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20060701_20201231_07378.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_13669_20070901_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20070901_20201231_13669.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_13670_20070601_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20070601_20201231_13670.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_13671_20071201_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20071201_20201231_13671.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_13696_20071201_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20071201_20201231_13696.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_13700_20080501_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20080501_20201231_13700.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_13713_20071101_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20071101_20201231_13713.txt\n",
      "\n",
      "Zip archive: data/original/DWD//hourly/precipitation/historical/stundenwerte_RR_15000_20110401_20201231_hist.zip\n",
      "Extract product file: produkt_rr_stunde_20110401_20201231_15000.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#produce a very large txt file with only 3 rows\n",
    "csvfname = \"data/generated/precipitation_timeseriess_appended_3_cols.csv\" #localcopy ot TS table Station_id, timestamp and measurement values\n",
    "\n",
    "first = False\n",
    "for elt in local_zip_list:\n",
    "    ffname = local_ftp_ts_dir + elt\n",
    "    print(\"Zip archive: \" + ffname)\n",
    "    with ZipFile(ffname) as myzip:\n",
    "        # read the time series data from the file starting with \"produkt\"\n",
    "        prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "        print(\"Extract product file: %s\" % prodfilename)\n",
    "        print()\n",
    "        with myzip.open(prodfilename) as myfile:\n",
    "            dftmp = prec_ts_to_df(myfile)[[\"stations_id\",\"r1\"]]\n",
    "            # df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n",
    "            dftmp.rename(columns={'stations_id': 'station_id', 'r1': 'val', 'mess_datum': 'ts'}, inplace = True)\n",
    "            dftmp.rename_axis('ts', inplace = True)\n",
    "            # dftmp.to_csv(f, header=f.tell()==0)\n",
    "            if (first):\n",
    "                first = False\n",
    "                dftmp.to_csv(csvfname, mode = \"w\", header = True)\n",
    "            else:\n",
    "                dftmp.to_csv(csvfname, mode = \"a\", header = False)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-04-01 00:00:00+00:00</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-04-01 01:00:00+00:00</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-04-01 02:00:00+00:00</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-04-01 03:00:00+00:00</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-04-01 04:00:00+00:00</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31 19:00:00+00:00</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31 20:00:00+00:00</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31 21:00:00+00:00</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31 22:00:00+00:00</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31 23:00:00+00:00</th>\n",
       "      <td>15000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85512 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           station_id  val\n",
       "ts                                        \n",
       "2011-04-01 00:00:00+00:00       15000  0.0\n",
       "2011-04-01 01:00:00+00:00       15000  0.0\n",
       "2011-04-01 02:00:00+00:00       15000  0.0\n",
       "2011-04-01 03:00:00+00:00       15000  0.0\n",
       "2011-04-01 04:00:00+00:00       15000  0.0\n",
       "...                               ...  ...\n",
       "2020-12-31 19:00:00+00:00       15000  0.0\n",
       "2020-12-31 20:00:00+00:00       15000  0.1\n",
       "2020-12-31 21:00:00+00:00       15000  0.0\n",
       "2020-12-31 22:00:00+00:00       15000  0.1\n",
       "2020-12-31 23:00:00+00:00       15000  0.0\n",
       "\n",
       "[85512 rows x 2 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract product file: produkt_rr_stunde_20041001_20201231_00216.txt\n",
      "Extract product file: produkt_rr_stunde_20091101_20201231_00389.txt\n",
      "Extract product file: produkt_rr_stunde_20040701_20201231_00390.txt\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_00554.txt\n",
      "Extract product file: produkt_rr_stunde_20080101_20181101_00555.txt\n",
      "Extract product file: produkt_rr_stunde_19990303_20201231_00603.txt\n",
      "Extract product file: produkt_rr_stunde_20041101_20201231_00613.txt\n",
      "Extract product file: produkt_rr_stunde_20040601_20201231_00617.txt\n",
      "Extract product file: produkt_rr_stunde_20050101_20201231_00644.txt\n",
      "Extract product file: produkt_rr_stunde_20041101_20201231_00796.txt\n",
      "Extract product file: produkt_rr_stunde_20050801_20201231_00871.txt\n",
      "Extract product file: produkt_rr_stunde_20061001_20201231_00902.txt\n",
      "Extract product file: produkt_rr_stunde_20041001_20201231_00934.txt\n",
      "Extract product file: produkt_rr_stunde_20050201_20201231_00989.txt\n",
      "Extract product file: produkt_rr_stunde_20060801_20201231_01024.txt\n",
      "Extract product file: produkt_rr_stunde_20041001_20201231_01046.txt\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_01078.txt\n",
      "Extract product file: produkt_rr_stunde_20061201_20201231_01241.txt\n",
      "Extract product file: produkt_rr_stunde_20150801_20201231_01246.txt\n",
      "Extract product file: produkt_rr_stunde_20040601_20201231_01300.txt\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_01303.txt\n",
      "Extract product file: produkt_rr_stunde_20040801_20201231_01327.txt\n",
      "Extract product file: produkt_rr_stunde_20030701_20201231_01590.txt\n",
      "Extract product file: produkt_rr_stunde_20121001_20201231_01595.txt\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_01766.txt\n",
      "Extract product file: produkt_rr_stunde_20060601_20201231_02027.txt\n",
      "Extract product file: produkt_rr_stunde_20030105_20201231_02110.txt\n",
      "Extract product file: produkt_rr_stunde_20050601_20201231_02254.txt\n",
      "Extract product file: produkt_rr_stunde_20041201_20201231_02473.txt\n",
      "Extract product file: produkt_rr_stunde_19951012_20201231_02483.txt\n",
      "Extract product file: produkt_rr_stunde_20040801_20201231_02497.txt\n",
      "Extract product file: produkt_rr_stunde_20040701_20201231_02629.txt\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_02667.txt\n",
      "Extract product file: produkt_rr_stunde_20061201_20201231_02810.txt\n",
      "Extract product file: produkt_rr_stunde_20061001_20201231_02947.txt\n",
      "Extract product file: produkt_rr_stunde_20081201_20201231_02968.txt\n",
      "Extract product file: produkt_rr_stunde_20040701_20201231_02999.txt\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_03028.txt\n",
      "Extract product file: produkt_rr_stunde_20040701_20201231_03031.txt\n",
      "Extract product file: produkt_rr_stunde_20071201_20201231_03081.txt\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_03098.txt\n",
      "Extract product file: produkt_rr_stunde_20070601_20201231_03215.txt\n",
      "Extract product file: produkt_rr_stunde_20050701_20201231_03321.txt\n",
      "Extract product file: produkt_rr_stunde_20060901_20201231_03339.txt\n",
      "Extract product file: produkt_rr_stunde_20060701_20201231_03499.txt\n",
      "Extract product file: produkt_rr_stunde_20041101_20201231_03540.txt\n",
      "Extract product file: produkt_rr_stunde_20040601_20201231_03591.txt\n",
      "Extract product file: produkt_rr_stunde_20041201_20201231_03795.txt\n",
      "Extract product file: produkt_rr_stunde_20040701_20201231_03913.txt\n",
      "Extract product file: produkt_rr_stunde_20030701_20201231_04063.txt\n",
      "Extract product file: produkt_rr_stunde_20050101_20201231_04127.txt\n",
      "Extract product file: produkt_rr_stunde_20051201_20201231_04150.txt\n",
      "Extract product file: produkt_rr_stunde_20110201_20180701_04154.txt\n",
      "Extract product file: produkt_rr_stunde_20040801_20201231_04313.txt\n",
      "Extract product file: produkt_rr_stunde_20041001_20201231_04368.txt\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_04371.txt\n",
      "Extract product file: produkt_rr_stunde_20040801_20201231_04400.txt\n",
      "Extract product file: produkt_rr_stunde_20060801_20201231_04488.txt\n",
      "Extract product file: produkt_rr_stunde_20080301_20181201_04692.txt\n",
      "Extract product file: produkt_rr_stunde_20041201_20201231_04741.txt\n",
      "Extract product file: produkt_rr_stunde_20050601_20201231_04849.txt\n",
      "Extract product file: produkt_rr_stunde_20041201_20201231_05064.txt\n",
      "Extract product file: produkt_rr_stunde_19950901_20201231_05347.txt\n",
      "Extract product file: produkt_rr_stunde_20070701_20201231_05360.txt\n",
      "Extract product file: produkt_rr_stunde_20060701_20201001_05468.txt\n",
      "Extract product file: produkt_rr_stunde_20030910_20201231_05480.txt\n",
      "Extract product file: produkt_rr_stunde_20050901_20201231_05513.txt\n",
      "Extract product file: produkt_rr_stunde_20041201_20201231_05619.txt\n",
      "Extract product file: produkt_rr_stunde_20041101_20201231_05699.txt\n",
      "Extract product file: produkt_rr_stunde_20060901_20201231_05717.txt\n",
      "Extract product file: produkt_rr_stunde_20050501_20201231_05733.txt\n",
      "Extract product file: produkt_rr_stunde_20001013_20201231_06197.txt\n",
      "Extract product file: produkt_rr_stunde_20040601_20201231_06264.txt\n",
      "Extract product file: produkt_rr_stunde_20041101_20191102_06276.txt\n",
      "Extract product file: produkt_rr_stunde_20041201_20201231_06313.txt\n",
      "Extract product file: produkt_rr_stunde_20040801_20201231_06337.txt\n",
      "Extract product file: produkt_rr_stunde_20060901_20201231_07106.txt\n",
      "Extract product file: produkt_rr_stunde_20051001_20201231_07330.txt\n",
      "Extract product file: produkt_rr_stunde_20060601_20201231_07344.txt\n",
      "Extract product file: produkt_rr_stunde_20060301_20201231_07374.txt\n",
      "Extract product file: produkt_rr_stunde_20060701_20201231_07378.txt\n",
      "Extract product file: produkt_rr_stunde_20070901_20201231_13669.txt\n",
      "Extract product file: produkt_rr_stunde_20070601_20201231_13670.txt\n",
      "Extract product file: produkt_rr_stunde_20071201_20201231_13671.txt\n",
      "Extract product file: produkt_rr_stunde_20071201_20201231_13696.txt\n",
      "Extract product file: produkt_rr_stunde_20080501_20201231_13700.txt\n",
      "Extract product file: produkt_rr_stunde_20071101_20201231_13713.txt\n",
      "Extract product file: produkt_rr_stunde_20110401_20201231_15000.txt\n",
      "create index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x2c1866c71c0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inserts the data directly into postgresql #better option than to create a text file locally\n",
    "first = True\n",
    "\n",
    "dtypes = {\"station_id\": Numeric(6,0), \"val\" : REAL}\n",
    "\n",
    "#for elt in local_zip_list[0:1]:\n",
    "for elt in local_zip_list:\n",
    "    ffname = local_ftp_ts_dir + elt\n",
    "    #print(\"Zip archive: \" + ffname)\n",
    "    with ZipFile(ffname) as myzip:\n",
    "        # read the time series data from the file starting with \"produkt\"\n",
    "        prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "        print(\"Extract product file: %s\" % prodfilename)\n",
    "        # print()\n",
    "        with myzip.open(prodfilename) as myfile:\n",
    "            dftmp = prec_ts_to_df(myfile)[[\"stations_id\",\"r1\"]]\n",
    "            # df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n",
    "            dftmp.rename(columns={'stations_id': 'station_id', 'r1': 'val', 'mess_datum': 'ts'}, inplace = True)\n",
    "            dftmp.rename_axis('ts', inplace = True)\n",
    "            # dftmp.to_csv(f, header=f.tell()==0)\n",
    "            if (first):\n",
    "                first = False\n",
    "                # dftmp.to_csv(csvfname, mode = \"w\", header = False)\n",
    "                dftmp.to_sql(name=\"prec\", schema=\"dwd\", if_exists = \"replace\", index = [\"ts\"], index_label=True, con=engine, dtype=dtypes)\n",
    "            else:\n",
    "                # dftmp.to_csv(csvfname, mode = \"a\", header = False)\n",
    "                dftmp.to_sql(name=\"prec\", schema=\"dwd\", if_exists = \"append\",  index = [\"ts\"], index_label=True, con=engine, dtype=dtypes)\n",
    "\n",
    "# After insert completed: ceate index\n",
    "print(\"create index\")\n",
    "engine.execute(\"ALTER TABLE dwd.prec ADD PRIMARY KEY (ts, station_id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x183f37a1120>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"CREATE OR REPLACE VIEW dwd.v_stations_prec as (select t1.station_id, t2.ts, t2.val, t1.geometry from dwd.stations t1, dwd.prec t2 where t1.station_id = t2.station_id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['builtins',\n",
       " 'builtins',\n",
       " 'os',\n",
       " 'ftplib',\n",
       " 'pandas',\n",
       " 'codecs',\n",
       " 'fiona',\n",
       " 'geopandas',\n",
       " 'types']"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import types\n",
    "def imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            yield val.__name__\n",
    "list(imports())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
